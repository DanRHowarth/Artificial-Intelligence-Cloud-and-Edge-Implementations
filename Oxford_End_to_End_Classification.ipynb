{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oxford_End_to_End_Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanRHowarth/Artificial-Intelligence-Cloud-and-Edge-Implementations/blob/master/Oxford_End_to_End_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iVt2_KcpLSX",
        "colab_type": "text"
      },
      "source": [
        "# End to End ML Problem: Classification\n",
        "\n",
        "* This is a `binary classification` problem, as we are training a model to differentiate between two different target classes, or categories. This is different from a `mutli-class classification` problem, which has more than two classes. The principles are the same, but the algorithms we use and the evaluation metrics can be different deoending on the type of problem.\n",
        "* We will tackle this problem using the same ML workflow as before; we will introduce you to the key libraries and algorithms; and we will provide code that you can use yourself. So let's begin wiht a recap of the `machine learning workflow` before moving on to the libraries we will use and the problem we will solve. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cy6gO2H7oL3E",
        "colab_type": "text"
      },
      "source": [
        "## RECAP: Machine Learning Workflow\n",
        "\n",
        "![alt text](https://github.com/DanRHowarth/Artificial-Intelligence-Cloud-and-Edge-Implementations/blob/master/Screenshot%202019-11-11%20at%2022.01.44.png?raw=true)\n",
        "\n",
        "### 1. Define the Problem Statement\n",
        "\n",
        "### 2. Analyze and Preprocess Data\n",
        "\n",
        "### 3. Split the data set \n",
        "\n",
        "### 4. Choose the most appropriate baseline algorithm\n",
        "\n",
        "### 5. Train and test your baseline model\n",
        "\n",
        "### 6. Chose quality evaluation metric(s)\n",
        "\n",
        "### 7. Refine our dataset to improve the baseline model\n",
        "\n",
        "### 8. Test alternative models\n",
        "\n",
        "### 9. Choose the best model and optimize it's parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c74xIXyTrdYu",
        "colab_type": "text"
      },
      "source": [
        "## RECAP: Key `python` libraries \n",
        "\n",
        "* We will use the following `python` libraries, which you will encounter frequently for data analysis and machine learning tasks: \n",
        " *  `numpy`, which provides vectorised arrays, and maths, algebra functionality;\n",
        " * `pandas`, which provides data structures and data analysis tools;\n",
        " * `matplotlib`, which provides highly customisable plotting functionality (and we also use `seaborn`, built on top of `matplotlib`, which is less customisable but can generate charts with less code); and, \n",
        " * `scikit-learn`, which provides models and tools for most machine learning algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMCx7d_L8zKS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import main data analysis libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "# note we use scipy for generating a uniform distribution in the model optimization step\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# note that because of the different dataset and algorithms, we use different sklearn libraries from Day 1 \n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# hide warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1P0G8eH8zKN",
        "colab_type": "text"
      },
      "source": [
        "## Machine Learning Workflow in Action\n",
        "* To take you through a machine learning problem following the steps introduced earlier \n",
        "* To explain at a high level the different steps, and the code used to implement them\n",
        "* To introduce new concepts as and when required"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2zMz-_T2kDg",
        "colab_type": "text"
      },
      "source": [
        "### 1. Define the Problem Statement\n",
        "* For our second end to end problem, we will use anoterh dataset that is provided as part of the `sci-kit learn` library, the breast cancer dataset. We can load this dataset easily and see a general description of what it contains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbYVzsB88zKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we load the dataset and save it as the variable data\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# if we want to know what sort of detail is provided with this dataset, we can call .keys()\n",
        "data.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jZS7oKp8zKf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the info at the .DESCR key will tell us more \n",
        "print (data.DESCR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aj4cWrzV8zKj",
        "colab_type": "text"
      },
      "source": [
        "* There is a lot of information to inform our problem statement. We have been given a `data` and `target` set of values and told that the target values are two classes - Malignant and Benign. We also know there are 212 Malignant and 357 Benign values and associated features. \n",
        "* We can therefore say that the problem statement is *to predict the class given a set number of features*. \n",
        "* We have 30  features. We will use those eatures and associated target variable  to train a model using seen or training data. We will then feed our trained model the 30 features from unseen or test data *without* passing in the target variable information. Our model will predict a class based on these features and we will compare that prediction to the actual answer to assess how well our model performs. \n",
        "* We can also see that our data set contains 569 records. It has no missing values, which is unusual but makes our job of `preprocessing` easier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niGQMTjo4TcV",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Analyse and Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6IrqilK58Wq",
        "colab_type": "text"
      },
      "source": [
        "#### Analyze the Data\n",
        "* We will use `pandas` and `matplotlib` to do some basic `exploratory data analysis`\n",
        "* This will include getting a feel for the overall dataset so that we understand what sorts of values it contains\n",
        "* We will compute summary statistics and look at the distributions of each feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gka2uPfu8zKk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can use pandas to create a dataframe, which is basically a way of storing and operating on tabular data \n",
        "# here we pass in both the data and the column names as variables\n",
        "X = pd.DataFrame(data.data, columns = data.feature_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGdGERpF8zKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can then look at the top of the dataframe to see the sort of values it contains\n",
        "X.describe(include = 'all')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuUYxEZt6LS-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can now look at our target variable \n",
        "y = data.target\n",
        "\n",
        "# we can see that it is a list of 0s and 1s, with 1s matching to the Benign class\n",
        "y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bPiA7aM8zKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can analyse the data in more detail by understanding how the features and target variables interact\n",
        "# we can do this by grouping the features and the target into the same dataframe\n",
        "# note we create a copy of the data here so that any changes don't impact the original data\n",
        "\n",
        "full_dataset = X.copy()\n",
        "full_dataset['target'] = y.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG02TUQp8zKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's take a look at the first few lines of the dataset\n",
        "full_dataset.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq3Tia2S8zK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets see how balanced the classes are (and if that matches to our expectation)\n",
        "full_dataset['target'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfBu2_nUzagv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's evaluate visually how well our classes are differentiable on the pairplots\n",
        "# can see two classes being present on a two variables charts?\n",
        "# the pairplot function is an excellent way of seeing how variables inter-relate, but 30 feature can make studying each combination difficult!\n",
        "sns.pairplot(full_dataset, hue = 'target')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU7OJqZQ0sWy",
        "colab_type": "text"
      },
      "source": [
        "* We can clearly see the presence of two clouds with different colors, representing our target classes. \n",
        "* Of course, they are still mixed to some extent, but if we were to visualise the variables in multi-dimentional space they would become more separable.\n",
        "* Now let's check the Pearson's correlation between pairs of our features and also between the features and our target. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F15Oakqg8zK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can again use seaborn to easily create a visually interesting chart \n",
        "plt.figure(figsize = (15, 10))\n",
        "\n",
        "# we can add the annot=True parameter to the sns.heatmap arguments if we want to show the correlation values \n",
        "sns.heatmap(full_dataset.corr(method='pearson'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEHTPhRx8zLE",
        "colab_type": "text"
      },
      "source": [
        "* Dark red colours are positilvey correlated with the corresponding feature, dark blue features are negatively correlated.\n",
        "* We can see that some values are negatively correlated with our target variable.\n",
        "* This information could help us with feature engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swrA1g2u8zLH",
        "colab_type": "text"
      },
      "source": [
        "###  Step 3: Split the data\n",
        "* In order to train our model and see how well it performs, we need to split our data into training and testing sets.\n",
        "* We can then train our model on the training set, and test how well it has generalised to the data on the test set.\n",
        "* There are a number of options for how we can split the data, and for what proportion of our original data we set aside for the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wLr8bjx8zLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Because our classes are not absolutely equal in number, we can apply stratification to the split \n",
        "# and be sure that the ratio of the classes in both train and test will be the same \n",
        "# you can learn about the other parameters by looking at the documentation \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCI3PgrC2dkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# as with Day 1, we can get shape of test and training sets\n",
        "print('Training Set:')\n",
        "print('Number of datapoints: ', X_train.shape[0])\n",
        "print('Number of features: ', X_train.shape[1])\n",
        "print('\\n')\n",
        "print('Test Set:')\n",
        "print('Number of datapoints: ', X_test.shape[0])\n",
        "print('Number of features: ', X_test.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkMZzAmn16hw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and we can verify the stratifications using np.bincount\n",
        "print('Labels counts in y:', np.bincount(y))\n",
        "print('Percentage of class zeroes in class_y',np.round(np.bincount(y)[0]/len(y)*100))\n",
        "\n",
        "print(\"\\n\")\n",
        "print('Labels counts in y_train:', np.bincount(y_train))\n",
        "print('Percentage of class zeroes in y_train',np.round(np.bincount(y_train)[0]/len(y_train)*100))\n",
        "\n",
        "print(\"\\n\")\n",
        "print('Labels counts in y_test:', np.bincount(y_test))\n",
        "print('Percentage of class zeroes in y_test',np.round(np.bincount(y_test)[0]/len(y_test)*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYEvThol8zLO",
        "colab_type": "text"
      },
      "source": [
        "###  Step 4: Choose a Baseline algorithm\n",
        "* Building a model in `sklearn` involves:\n",
        "  * defining / instantiating the model we want to use and its parameters (**Step 4**)\n",
        "  * fitting the model we have developed to our training set (**Step 5**)\n",
        "* We can then use the model to predict scores against our test set and assess how good it is\n",
        "* To do this, we need to define an evaluation metric (**Step 6**). There are a number of different options, and they differ for both regression and classification problems. This score will be what we use to select our best model, and the best parameters. \n",
        "* We will take through these steps now. As you will see, the code required to implement these steps is minimal, thanks to different methods provided for us by  `sklearn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iJ3Qj0WF4C2p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## we can create a baseline model to benchmark our other estimators against\n",
        "## this can be a simple estimator or we can use a dummy estimator to make predictions in a random manner \n",
        "\n",
        "# this creates our dummy classifier, and the value we pass in to the strategy parameter dtermn\n",
        "dummy = DummyClassifier(strategy='uniform', random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzYe_Sgs5gOa",
        "colab_type": "text"
      },
      "source": [
        "### Step 5: Train and Test the Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMqsQ0DM4I4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \"Train\" model\n",
        "dummy.fit(X_train, y_train)\n",
        "\n",
        "# from this, we can generate a set of predictions on our unseen features, X_test\n",
        "dummy_predictions = dummy.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkVPrSBh8zLX",
        "colab_type": "text"
      },
      "source": [
        "###  Step 6: Choose an evaluation metric\n",
        "* We then need to compare these predictions with the actual result and measure them in some way. This is where the selection of evaluation metric is important. \n",
        "* Classification metrics include:\n",
        "  * `accuracy`: this assess how often the model selects the best class. This can be more useful when there are balanced classes (i.e. there are a similar number of instances of each class we are trying to predict). \n",
        "    * There are some limits to this metric. For example, if we think about something like credit card fraud, where the instances of fraudulent transactions might be 0.5%, then a model that *always* predicts that a transaction is not fraudulent will be 99.5% accurate! So we often need metrics that can tell us how a model performs in more detail. \n",
        "  * `f1 score`: \n",
        "  * `roc_auc`: \n",
        "  * `recall`: \n",
        "  * We recommend you research these metrics to improve your understanding of how they work. Try to look up an explanation or two (for example on wikipedia and scikit-learn documentation) and write a one line summary in the space provided above. Then, below, when we implement a scoring function, select these different metrics and try to explain what is happening. This will help cement you knowledge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX1eBpfJ6T-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(y_test, y_pred):\n",
        "    # this block of code returns all the metrics we are interested in \n",
        "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
        "    f1 = metrics.f1_score(y_test, y_pred)\n",
        "    auc = metrics.roc_auc_score(y_test, y_pred)\n",
        "\n",
        "    print (\"Accuracy\", accuracy)\n",
        "    print ('F1 score: ', f1)\n",
        "    print ('ROC_AUC: ' , auc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBsEjVjz8MRz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can call the function on the actual results versus the predictions\n",
        "# we will see that the metrics are what we'd expect from a random model \n",
        "evaluate(y_test, dummy_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djVOMf3R7F8m",
        "colab_type": "text"
      },
      "source": [
        "### Step 7: Refine our dataset\n",
        "* In Day 1, we undertook some featuring engineering to try and make the dataset more representative of the problem we were trying to solve.\n",
        "* We will focus on other steps today, but please feel free to try this step in order to build your understanding of this technique"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h70MbMy27gTc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FEATURE ENGINEERING CODE HERE\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11WaAWLm7l3f",
        "colab_type": "text"
      },
      "source": [
        "### Step 8: Test Alternative Models\n",
        "* Once we got a nice baseline model working for this dataset, we also can try something more sophisticated and rather different. \n",
        "* As well as using a different estimator, we can use a different method, `cross_validation`, to generate a score. This is shown to broaden your knowledge.\n",
        "  * There are a number of different ways in `scikit-learn` to get an estimator score and it can get confusing first.\n",
        "  * Remember that to get a score, we need to instantiate a model, fit it to the data, predict using unseen data, compare the predictions against actual data, and score the difference. This is true for classification and regression problems, and is true no matter the method used to get there.\n",
        "    * So, in the end-to-end tutorials we split the training and test data,  fitted our data to an estimator, and called the `.predict` method on the estimator to get our predictions, and then passed this to a scoring function (four steps)\n",
        "    * We can use `estimator.score()`method, where we pass in our split data and the method then makes predictions and returning the score (three steps). \n",
        "    * And, in the `cross_val_score()` method used below we are effectively using one step as the method takes an estimator and our data and returns a score. You can find out more about this method [here](https://scikit-learn.org/stable/modules/cross_validation.html) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0hAAFLM-Pbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## here we fit a new estimator and use cross_val_score to get a score based on a defined metric \n",
        "\n",
        "# instantiate logistic regression classifier\n",
        "logistic = LogisticRegression()\n",
        "\n",
        "# we pass our estimator and data to the method. we also specify the number of folds (default is 3)\n",
        "# the default scoring method is the one associated with the estimator we pass in\n",
        "# we can use the scoring parameter to pass in different scoring methods. Here we use f1.  \n",
        "cross_val_score(logistic, X, y, cv=5, scoring=\"f1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrfmMtk7_o2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can see that this returns a score for all the five folds of the cross_validation\n",
        "# if we want to return a mean, we can store as a variable and calculate the mean, or do it directly on the function\n",
        "# this time we will use accuracy\n",
        "cross_val_score(logistic, X, y, cv=5, scoring=\"accuracy\").mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taVq_2SLFaeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets do this again with a different model\n",
        "rnd_clf = RandomForestClassifier()\n",
        "\n",
        "# and pass that in \n",
        "cross_val_score(rnd_clf, X, y, cv=5, scoring=\"accuracy\").mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sz-7ZUkQEpYF",
        "colab_type": "text"
      },
      "source": [
        "#### Ensemble models \n",
        "\n",
        "* Let's take this opportunity to explore ensemble methods.\n",
        "* The goal of ensemble methods is to combine different classifiers into a meta-classifier that has better generalization performance than each individual classifier alone. \n",
        "* There are several different approaches to achieve this, including **majority voting** ensemble methods, which we select the class label that has been predicted by the majority of classifiers.\n",
        "* The ensemble can be built from different classification algorithms, such as decision trees, support vector machines, logistic regression classifiers, and so on. Alternatively, we can also use the same base classification algorithm, fitting different subsets of the training set. \n",
        "* Indeed, Majority voting will work best if the classifiers used are different from each other and/or trained on different datasets (or subsets of the same data) in order for their errors to be uncorrelated. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrCA92LvEyG6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets instantiate an additional model to make an ensemble of three models\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "\n",
        "# and an ensemble of them\n",
        "voting_clf = VotingClassifier(estimators=[('lr', logistic), ('rf', rnd_clf), ('dc', dt_clf)],                              \n",
        "                              # here we select soft voting, which returns the argmax of the sum of predicted probabilities\n",
        "                              voting='soft')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rKI0NOUFObi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# here we can cycle through the individual estimators \n",
        "# for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
        "\n",
        "for clf in (logistic, rnd_clf, dt_clf, voting_clf):\n",
        "    \n",
        "    # fit them to the training data \n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    # get a prediction\n",
        "    y_pred = clf.predict(X_test)\n",
        "    \n",
        "    # and print the prediction \n",
        "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bErndFEbGCjC",
        "colab_type": "text"
      },
      "source": [
        "* We can see that `voting classifier` in this the case does have a slight edge on the other models (note that this could vary depending on how the data is split at training time).\n",
        "* This is an interesting approach and one to consider when you are developing your models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gpAiep58zLn",
        "colab_type": "text"
      },
      "source": [
        "### Step 9: Choose the best model and optimise its parameters\n",
        "* We can see that we have improved our model as we have added features and trained new models.\n",
        "* At the point that we feel comfortable with a good model, we can start to tune the parameters of the model.\n",
        "* There are a number of ways to do this. We applied `GridSearchCV` to identify the best hyperparameters for our models on Day 1.\n",
        "* There are other methods available to use that don't take the brute force approach of `GridSearchCV`.\n",
        "* We will cover an implementation of `RandomizedSearchCV` below, and use the exercise for you to implement it on the other datatset.\n",
        "* We use this method to search over defined hyperparameters, like `GridSearchCV`, however a fixed number of parameters are sampled, as defined by `n_iter` parameter.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V02V0MhYA3Ne",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will optimise logistics regression \n",
        "# we can create hyperparameters as a list, as in type regularization penalty \n",
        "penalty = ['l1', 'l2']\n",
        "\n",
        "# or as a distribution of values to sample from -'C' is the hyperparameter controlling the size of the regularisation penelty \n",
        "C = uniform(loc=0, scale=4)\n",
        "\n",
        "# we need to pass these parameters as a dictionary of {param_name: values}\n",
        "hyperparameters = dict(C=C, penalty=penalty)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLObVsu_BVTR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we instantiate our model\n",
        "randomizedsearch = RandomizedSearchCV(logistic, hyperparameters, random_state=1, \n",
        "                                      n_iter=100, cv=5, verbose=0, n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqlVBZ9LBis1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and fit it to the data \n",
        "best_model = randomizedsearch.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCShyH69Bk-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and we can call this method to return the best parameters the search returned\n",
        "best_model.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04IPoZAcIFmN",
        "colab_type": "text"
      },
      "source": [
        "#### Using `cross_val_score` to tune hyperparameters\n",
        "* You may have noticed that we are unable to evaluate the performance of  `best_model` on unseen data because we have used the dataset for the `RandomizedSearchCV` implementation. \n",
        "* We could look to use just our `X_train` and `y_train` dataset in `randomizedsearch`and then evaluate on our test sets. \n",
        "* Or we could use `cross_val_score`, as we did above. In this instance, it will create a nested cross-validation set, effectively keeping part of the data unseen so we can test it `k` number of times. We will be therefore be able to return a model score and evaluate the model's performance.\n",
        "* See section 3.7 of [this](https://sebastianraschka.com/pdf/manuscripts/model-eval.pdf) for more detail (and an excellent discussion of model selection overall)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGsFPpY4B-uA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## here we are effectively running the randomizedsearch function 5 times, each time with some data kept aside for evaluation\n",
        "## this will take a few minutes to run!\n",
        "cross_val_score(randomizedsearch, X, y, cv=5, scoring=\"accuracy\").mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq3HB6CRAFh3",
        "colab_type": "text"
      },
      "source": [
        "## Summary and Next Steps:\n",
        "* Thank you so much for joining these las two days. We hope that you learnt a lot and that you have been excited enough to continue your machine learning journey.\n",
        "* Don't worry if there are some topics that you didn't fully understand. This is a broad, ever-evolving field. Try to focus on developing some specific knowledge of the areas you need most development in - but remember to also relate it to the broader picture to cement your understanding. \n",
        "* Please also leave comments on the page where these notebooks where published and we will try to answer them as soon as we can.\n"
      ]
    }
  ]
}