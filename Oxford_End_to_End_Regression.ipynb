{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Oxford_End_to_End_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanRHowarth/Artificial-Intelligence-Cloud-and-Edge-Implementations/blob/master/Oxford_End_to_End_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WSqj0N9Hg96",
        "colab_type": "text"
      },
      "source": [
        "# Artificial-Intelligence-Cloud-and-Edge-Implementations: \n",
        "# End-to-End Machine Learning Problem: Regression\n",
        "\n",
        "* The aim of this notebook is to introduce you to the core concepts in developing and building a machine learning model, including:\n",
        "\n",
        "  * An overview of Machine Learning; \n",
        "  * An overview of the the Machine Learning workflow; \n",
        "  * An introduction, through explanations and code, of the key machine learning libraries in `python`; and\n",
        "  * Familiariry with the `collaboratory` environment \n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUdUIdMKNNNO",
        "colab_type": "text"
      },
      "source": [
        "## CONCEPT 1: What is Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaPrvK2MYO8C",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Aims:\n",
        "  * To introduce the concept of machine learning \n",
        "  * To introduce the different types of machine learning\n",
        "  * To suggest further reading in order to explore this concept in more detail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8BmebTwGIhS",
        "colab_type": "text"
      },
      "source": [
        "### Explanation: \n",
        "\n",
        "**Machine Learning**\n",
        "* From ['Python Data Science Handbook'](https://jakevdp.github.io/PythonDataScienceHandbook/) by Jake VanderPlass:\n",
        "\n",
        "* *Machine learning is often categorized as a subfield of artificial intelligence, but I find that categorization can often be misleading at first brush. The study of machine learning certainly arose from research in this context, but in the data science application of machine learning methods, it’s more helpful to think of **machine learning as a means of building models of data**.*\n",
        "\n",
        "\n",
        "* *Fundamentally, **machine learning involves building mathematical models to help understand data**. “Learning” enters the fray when we give these models tunable parameters that can be adapted to observed data; in this way the program can be considered to be “learning” from the data. Once these models have been fit to previously seen data, they can be used to predict and understand aspects of newly observed data.*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Different types of Machine Learning**\n",
        "\n",
        "* *Supervised learning* is about modeling the relationship between features of data and a label associated with the data. Once this model is determined, it can be used to apply labels to new, unknown data. \n",
        "* Supervised learning can either be a *classification* task (where our labels are in discrete categories) or *regression* task (where our labels are continuous quantities). \n",
        "* *Unsupervised learning* models the features of a dataset without reference to any label. These models include tasks such as clustering and dimensionality reduction. We can use unsupervised learning technqiues to preprocess data prior to running supervised learning models. \n",
        "* *Semi-supervised learning* can also be used when we only have incomplete labels for our data.\n",
        "* *Deep learning* can be thought of as a way of implementimg  *supervised*, *unsupervised* and *semi-supervised* learning using models that have more layers (we will come to Deep Learning later in the module)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNb73s9cGM19",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Next Steps:\n",
        "\n",
        "* For a good introduction, we recommend ['Python Data Science Handbook'](https://jakevdp.github.io/PythonDataScienceHandbook/), which available free at the link. \n",
        "\n",
        "* For a more detailed overview, we suggest Chapter 2 of *Introduction to Statistical Learning* (also free [here](http://www-bcf.usc.edu/~gareth/ISL/))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpnIe4L5JYK1",
        "colab_type": "text"
      },
      "source": [
        "## CONCEPT 2: Machine Learning Workflow\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Zw_8VZYae9",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/DanRHowarth/Artificial-Intelligence-Cloud-and-Edge-Implementations/blob/master/Screenshot%202019-11-11%20at%2022.01.44.png?raw=true)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFhBlcuzQUk5",
        "colab_type": "text"
      },
      "source": [
        "### 2.4 Next Steps:\n",
        "* There are various approaches to machine learning workflow, though all will include the areas covered above. \n",
        "* Good discussions of machine learning workflow are available. See *Python Machine Learning*', by Sebastian Raschka (see [here](https://sebastianraschka.com/books.html)) and this [blog](https://www.kdnuggets.com/2018/05/general-approaches-machine-learning-process.html) for  examples. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV41up0tRwlb",
        "colab_type": "text"
      },
      "source": [
        "## CONCEPT 3: Key `python` libraries \n",
        "**Aims:**\n",
        "* To introduce you to the key machine learning libraries\n",
        "* To import all the libraries we will need "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AztvHnFSo3m",
        "colab_type": "text"
      },
      "source": [
        "**Explanation:**\n",
        "* We will use the following `python` libraries, which you will encounter frequently for data analysis and machine learning tasks: \n",
        " *  `numpy`, which provides vectorised arrays, and maths, algebra functionality;\n",
        " * `pandas`, which provides data structures and data analysis tools;\n",
        " * `matplotlib`, which provides highly customisable plotting functionality (and we also use `seaborn`, built on top of `matplotlib`, which is less customisable but can generate charts with less code); and, \n",
        " * `scikit-learn`, which provides models and tools for most machine learning algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx1i0pKVRwlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# these are import statements \n",
        "# by convention, the libraries are imported as a shorthand that we will refer to \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# rather than importing the whole sklearn library, we will import certain modules \n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn import model_selection\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5Io9Kp_Ti2k",
        "colab_type": "text"
      },
      "source": [
        "**Next Steps:**\n",
        "* We have developed more focussed notebooks for each of the main libraries, and they are available as part of the course."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZkfBuy7UiE4",
        "colab_type": "text"
      },
      "source": [
        "## SECTION 1: Exploratory Data Analysis\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-eJQSklkYjj",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Aims:\n",
        "* To take you through the first part of a machine learning problem following the steps introduced earlier \n",
        "* To explain at a high level the different steps, and the code used to implement them"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frpkYkXykP9v",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Machine Learning Workflow - EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMuDW_D4k_3G",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/DanRHowarth/Artificial-Intelligence-Cloud-and-Edge-Implementations/blob/master/Screenshot%202019-11-11%20at%2022.02.10.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plYKMy8UkgoX",
        "colab_type": "text"
      },
      "source": [
        "#### 1. Define the Problem Statement\n",
        "* We will define the problem(s) that we are trying to solve given the data available to us\n",
        "* Note that steps 1 and 2 can be iterative, in that we may only know the problem we are trying to solve once we have analyzed the data. \n",
        "\n",
        "#### 2. Analyze and Preprocess Data\n",
        "* We will `analyze` our data to ensure we understand what information it contains, and whether some parts of the data are more relevant than others \n",
        "* We will also `preprocess` the data, so that any missing values are treated appropriately. We might `normalize` the data and turn `categorical` data into something that can be processed by our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B71_XzflRwlf",
        "colab_type": "text"
      },
      "source": [
        "### Step 1: Define the Problem Statement\n",
        "\n",
        "* For our first end to end problem, we will use a dataset that is provided as part of the `sci-kit learn` library, the Boston Houses dataset. We can load this dataset easily and see a general description of what it contains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t90YYSMyRwlg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we load the dataset and save it as the variable boston\n",
        "boston = load_boston()\n",
        "\n",
        "# if we want to know what sort of detail is provided with this dataset, we can call .keys()\n",
        "boston.keys()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh-KRuwjRwln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the info at the .DESCR key will tell us more \n",
        "print(boston.DESCR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohv11mdyRwlu",
        "colab_type": "text"
      },
      "source": [
        "* There is a lot of information to inform our problem statement. We have been given a `data` and `target` set of values and told that `MEDV` (median value) is usually used as the target variable. \n",
        "* We can therefore say that the problem statement is *to predict the median value of a home given a set number of features of that home*. \n",
        "* We have 13 other features. We will use those 13 features and accompanying target variable - the median value of the home - to train a model using seen or training data. We will then feed our trained model the 13 features from unseen or test data *without* passing in the target variable information. Our model will predict what it thinks is the median value and we will compare that prediction to the actual answer to assess how well our model performs. \n",
        "* We can also see that our data set contains 506 records. It has no missing values, which is unusual but makes our job of `preprocessing` easier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2EPZi9kWErG",
        "colab_type": "text"
      },
      "source": [
        "### Step 2: Analyse and Preprocess data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYwus1tKW0sZ",
        "colab_type": "text"
      },
      "source": [
        "#### Analyze the Data\n",
        "* We will use `pandas` and `matplotlib` to do some basic `exploratory data analysis`\n",
        "* This will include getting a feel for the overall dataset so that we understand what sorts of values it contains\n",
        "* We will compute summary statistics and look at the distributions of each feature\n",
        "* We can start to determine what are the most important features, where there might be `outliers` and whether we need to `normalize` the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oy4pgWhRwlv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can use pandas to create a dataframe, which is basically a way of storing and operating on tabular data \n",
        "# here we pass in both the data and the column names as variables\n",
        "boston_X = pd.DataFrame(boston.data, columns = boston.feature_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObNxe9QZQ9Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will also drop one of the features - B \n",
        "boston_X = boston_X.drop(columns='B')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiVTiekfRwlz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can then look at the top of the dataframe to see the sort of values it contains\n",
        "boston_X.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unZkKggTRwl9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pandas has a lot of functionality to assist with exploratory data analysis\n",
        "# .describe() provide summary statistics on all numeric columns\n",
        "print(boston_X.describe())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT6-tZXAYaHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can also see the shape of the data\n",
        "print(boston_X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfqRGDxjRwmB",
        "colab_type": "text"
      },
      "source": [
        "* For each feature, we can see the `count`, or number of data entries, the `mean` value, and the `standard deviation`, `min`, `max` and `quartile` values. \n",
        "* We can see that the range of values for each feature differs quite a lot, so we can start to think about whether to apply normalization to the data. \n",
        "* We can also see that the `CHAS` faeture is either a `(1,0)` value. If we look back at our description, we can see that this is an example of a `categorical` variable. These are values used to describe non-numeric data. In this case,  a `1` indicates the house borders near the river, and a `0` that it doesn't."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhYv8ivsRwmT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can build on this analysis by plotting the distribution and boxplots for each column\n",
        "\n",
        "# we loop through all the columns\n",
        "for col in boston_X.columns:\n",
        "    # and for each column we create space for one row with 2 charts \n",
        "    f, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    # our first chart is a histogram and we set the title \n",
        "    boston_X[col].hist(bins = 30, ax = axes[0])\n",
        "    axes[0].set_title('Distribution of '+ col)\n",
        "    # our second column is the boxplot \n",
        "    boston_X.boxplot(column = col, ax = axes[1])\n",
        "    # we then use this to command to display the charts\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CL3CcD-KwMk",
        "colab_type": "text"
      },
      "source": [
        "* A `histogram` tells is the number of times, or frequency, a value occurs within a `bin`, or bucket, that splits the data (and which we defined). A histogram shows the frequency with which values occur within each of these bins, and can tell us about the distribution of data. \n",
        "* A `boxplot` captures within the box the `interquartile range`, the range of values from Q1/25th percentile to Q3/75th percentile, and the median value. It also captures the `min` and `max` values of each feature.\n",
        "* Together, these charts show us the distribution of values for each feature. We can start to make judgements about how to treat the data, for example whether we want to deal with outliers; or whether we want to normalize the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEF0gQVARwl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can now look at our target variable \n",
        "boston_y = boston.target"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ey1Bd3cRwmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can plot a histogram in a slightly different way \n",
        "plt.hist(boston_y, bins = 40)\n",
        "plt.title('Housing price distribution, $K')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVLEgbDQRwmK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and the same for the boxplot\n",
        "plt.boxplot(boston_y)\n",
        "plt.title('Box plot for housing price.')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLKEzT6cRwmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# another thing we can do is plot a boxplot of one variable against the target variable \n",
        "# it is interesting to see how house value distribution differs by CHAS, the categorical variable \n",
        "\n",
        "# here we create a grouped dataframe that includes the target variable\n",
        "grouped_df = boston_X.copy()   # note we create a copy of the data here so that any changes don't impact the original data\n",
        "grouped_df['target'] = boston_y.copy()\n",
        "\n",
        "# we then plot it here\n",
        "f, axes = plt.subplots(1, 1, figsize=(10, 5))   \n",
        "grouped_df.boxplot(column='target', by = 'CHAS', ax = axes)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAvzi4hVRwmj",
        "colab_type": "text"
      },
      "source": [
        "* The `interquartile range`for houses next to the river is higher than for those houses not next to the river, and the `min` and `max` values differ too.\n",
        "* This suggests this could be an important variable for us to include in our model, given that as it differs, the target value distribution changes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyVI1CCERwmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can extend this sort of analysis by creating a heatmap\n",
        "# this shows the correlation between the features and target\n",
        "\n",
        "# first we compute the correlation\n",
        "corr = grouped_df.corr(method='pearson')\n",
        "# and plot our figure size\n",
        "plt.figure(figsize = (15, 10))\n",
        "# and use seaborn to fill this figure with a heatmap\n",
        "sns.heatmap(corr, annot = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igkBCvFpRwms",
        "colab_type": "text"
      },
      "source": [
        "* We will let you review this heatmap to see what features are important for modelling and why. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gv0rCefPRwmX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# OPTIONAL: below is code that generate a pairplot using seaborn \n",
        "# look up what a pairplot is and see if you can interpret the output of the code below\n",
        "\n",
        "#sns.pairplot(grouped_df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lFYZkbvRwnC",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocess the data\n",
        "* We proprocess the data to ensure it is a suitable state for modelling. The sort of things that we do to preprocess the data includes:\n",
        "  * *Dealing with missing values*, where we identify what, if, any missing data we have and how to deal with it. For example, we may replace missing values with the mean value for that feature, or by the average of the neighbouring values. \n",
        "    * `pandas` has a number of options for filling in missing data that is worth exploring\n",
        "    * We can also use `k-nearest neighbour`to help us predict what the missing values should be, or `sklearn Imputer` function (amongst other ways)\n",
        "  * *Treat categorical values*, by converting them into a numerical representation that can be modelled.\n",
        "    * There are a number of different ways to do this in `sklearn` and `pandas`\n",
        "  * *Normalise the data*, for example by ensuring the data is, for example all on the scale (such as within two defined values); normally distributed; has a zero-mean, etc. This is sometimes necessary for the ML models to work, and can also help speed up the time it takes for the models to run.  \n",
        "    * Again, `sklearn` and `pandas` have in-built functions to help you do this.\n",
        "* In this notebook, we will look to remove `outliers`, which are values that might be erroneous and which can over-influence the model, and `normalize` the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me7QTjz5Rwmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lets start by removing outliers\n",
        "\n",
        "# here we define the columns where we have identified there could be outliers\n",
        "numeric_columns = ['CRIM', 'ZN', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT']\n",
        "\n",
        "# this function can be used on any dataset to return a list of index values for the outliers \n",
        "def get_outliers(data, columns):\n",
        "    # we create an empty list\n",
        "    outlier_idxs = []\n",
        "    for col in columns:\n",
        "        elements = data[col]\n",
        "        # we get the mean value for each column\n",
        "        mean = elements.mean()\n",
        "        # and the standard deviation of the column\n",
        "        sd = elements.std()\n",
        "        # we then get the index values of all values higher or lower than the mean +/- 2 standard deviations\n",
        "        outliers_mask = data[(data[col] > mean + 3*sd) | (data[col]  < mean  - 3*sd)].index\n",
        "        # and add those values to our list\n",
        "        outlier_idxs  += [x for x in outliers_mask]\n",
        "    return list(set(outlier_idxs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHyG6JKrRwm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we call the function we just created on the boston dataset\n",
        "boston_outliers = get_outliers(boston_X, numeric_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4fqbCRJRwm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and drop those values from our feature and target values\n",
        "boston_X = boston_X.drop(boston_outliers, axis = 0)\n",
        "boston_y = pd.DataFrame(boston_y).drop(boston_outliers, axis = 0).values.ravel()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdedvCKLRwm7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can check that this code has worked by looking at the shape of our data \n",
        "print (boston_X.shape)\n",
        "print (boston_y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Voepu7b504DC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can also create a function to normalize our data\n",
        "# first lets look at the data before normalisation\n",
        "boston_X[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G65uHpDoRwnE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function loops through columns in a data set and defines a predefined scaler to each\n",
        "def scale_numeric(data, numeric_columns, scaler):\n",
        "    for col in numeric_columns:\n",
        "        data[col] = scaler.fit_transform(data[col].values.reshape(-1, 1))\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwSyaoRWRwnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can now define the scaler we want to use and apply it to our dataset \n",
        "\n",
        "# a good exercise would be to research waht StandardScaler does - it is from the scikit learn library \n",
        "scaler = StandardScaler()\n",
        "boston_X = scale_numeric(boston_X, numeric_columns, scaler)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZBAAGOsXzJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# here we can see the result \n",
        "boston_X[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soT57bwIloeU",
        "colab_type": "text"
      },
      "source": [
        "## SECTION 2: Machine Learning Workflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqFpAGlfmFRi",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://github.com/DanRHowarth/Artificial-Intelligence-Cloud-and-Edge-Implementations/blob/master/Screenshot%202019-11-11%20at%2022.01.44.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uakxP6vlmW-",
        "colab_type": "text"
      },
      "source": [
        "### 3. Split the data set \n",
        "* We will split our data so that we can train our model using a `training set` and test it on a `testing set`, which has previously not been seen by our model.\n",
        "* We can do this in a variety of ways, from simply splitting the data 'manually' to using techniques such as cross-validation. Testing different approaches, including how much data to holdback for testing, is part of the trial and error of building a machine learning model. \n",
        "\n",
        "### 4. Choose the most appropriate baseline algorithm\n",
        "* We will develop a basic, baseline model, to compare our more sophsiticated models with. \n",
        "* In the real world, this could be the current way things are done, even if this is currently not a machine learning method - it is often good to compare a complex model to something more basic to see what gains are really being achieved. \n",
        "\n",
        "### 5. Train and test your baseline model\n",
        "* Given our baseline model and evaluation metric, we will train our model on our training data and test it on our testing data.\n",
        "\n",
        "### 6. Chose quality evaluation metric(s)\n",
        "* We will choose a method to assess our model, and use this to compare all the models we develop.  \n",
        "* There are often a number of different metrics available, so it is important to consider what works best for a given problem and optimise the model for that metric.\n",
        "\n",
        "### 7. Refine our dataset to improve the baseline model\n",
        "* We can use a process called `feature engineering` to improve how our dataset represents the problem we are trying to solve (for example by putting more emphasis on certain aspects of the data).\n",
        "* Feature engineering is a key part of machine learning. \n",
        "\n",
        "### 8. Test alternative models\n",
        "* Once we have a good baseline, we can test alternative models that may perform better. We will compare them using the results from the test dataset. \n",
        "\n",
        "### 9. Choose the best model and optimize it's parameters\n",
        "* Each model that we test will have a wide range of parameters that can be altered to change performance. \n",
        "* Once we have an idea of our best models, we can further refine our results by testing whether these parameters improve performance. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEkoVkHhRwnM",
        "colab_type": "text"
      },
      "source": [
        "####  Step 3: Split the data\n",
        "* In order to train our model and see how well it performs, we need to split our data into training and testing sets.\n",
        "* We can then train our model on the training set, and test how well it has generalised to the data on the test set.\n",
        "* There are a number of options for how we can split the data, and for what proportion of our original data we set aside for the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj-Kx5NKRwnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a common way for splitting our dataset is using train_test_split \n",
        "\n",
        "# as an exercise, go to the scikit learn documentation to learn more about this function and the parameters available \n",
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(boston_X, boston_y, test_size = 0.2, random_state = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wXJ5tHXE1Hy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get shape of test and training sets\n",
        "print('Training Set:')\n",
        "print('Number of datapoints: ', X_train.shape[0])\n",
        "print('Number of features: ', X_train.shape[1])\n",
        "print('\\n')\n",
        "print('Test Set:')\n",
        "print('Number of datapoints: ', X_test.shape[0])\n",
        "print('Number of features: ', X_test.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKsLhTO9RwnT",
        "colab_type": "text"
      },
      "source": [
        "###  Step 4: Choose a Baseline algorithm\n",
        "* Building a model in `sklearn` involves:\n",
        "  * defining / instantiating the model we want to use and its parameters (**Step 4**)\n",
        "  * fitting the model we have developed to our training set (**Step 5**)\n",
        "* We can then use the model to predict scores against our test set and assess how good it is\n",
        "* To do this, we need to define an evaluation metric (**Step 6**). There are a number of different options, and they differ for both regression and classification problems. This score will be what we use to select our best model, and the best parameters. \n",
        "* We will take through these steps now. As you will see, the code required to implement these steps is minimal, thanks to different methods provided for us by  `sklearn`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UyfXHZ5ORwnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we instantiate a model by storing it as a variable \n",
        "# this is the same process for most of the algorithms available in scikit learn (though you will need to import different libraries for different algorithms)\n",
        "\n",
        "# linear regression is a fairly simple algorithm compared to more complicate regression options, so provides a good baseline\n",
        "lm = LinearRegression()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TlmWZq1Rwni",
        "colab_type": "text"
      },
      "source": [
        "### Step 5: Train and Test the Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuN3ot2LRwnj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fitting the model to the data means to train our model on the data\n",
        "# the fit function takes both the X and y variables of the training data \n",
        "lm.fit(X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuQeySC1II07",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from this, we can generate a set of predictions on our unseen features, X_test\n",
        "Y_pred = lm.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOc-7BGoRwne",
        "colab_type": "text"
      },
      "source": [
        "###  Step 6: Choose an evaluation metric\n",
        "* We then need to compare these predictions with the actual result and measure them in some way.\n",
        "* This is where the selection of evaluation metric is important. For regression, we measure the distance between the predicted and actual answers in some way. The shorter the distance, the more correct the model is. \n",
        "* We cover three common metrics below:\n",
        "  * `Mean Absolute Error`: which provides a mean score for all the predicted versus actual values as an absolute value \n",
        "  * `Means Squared Error`: which provides a mean score for all the predicted versus actual values as a square of the absolute value\n",
        "  * `R2`: which we recommend you research as an exercise to grow your knowledge. WIkipedia and `sklearn` document are a great place to start!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTmXoHJhRwng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(Y_test, Y_pred):\n",
        "    # this block of code returns all the metrics we are interested in \n",
        "    mse = metrics.mean_squared_error(Y_test, Y_pred)\n",
        "    msa = metrics.mean_absolute_error(Y_test, Y_pred)\n",
        "    r2 = metrics.r2_score(Y_test, Y_pred)\n",
        "\n",
        "    print(\"Mean squared error: \", mse)\n",
        "    print(\"Mean absolute error: \", msa)\n",
        "    print(\"R^2 : \", r2)\n",
        "    \n",
        "    # this creates a chart plotting predicted and actual \n",
        "    plt.scatter(Y_test, Y_pred)\n",
        "    plt.xlabel(\"Prices: $Y_i$\")\n",
        "    plt.ylabel(\"Predicted prices: $\\hat{Y}_i$\")\n",
        "    plt.title(\"Prices vs Predicted prices: $Y_i$ vs $\\hat{Y}_i$\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Xbr5cSiRwnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate(Y_test, Y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpUl5scCKZ3o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can explore how metrics are dervied in a little more detail by looking at MAE\n",
        "# here we will implement MAE using numpy, building it up step by step\n",
        "\n",
        "# with MAE, we get the absolute values of the error - as you can see this is of the difference between the actual and predicted values\n",
        "np.abs(Y_test - Y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYIgygxBK9tZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we will then sum them up \n",
        "np.sum(np.abs(Y_test - Y_pred))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gByAtaoLHsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# then divide by the total number of predictions/actual values\n",
        "# as you will see, we get to the same score implemented above \n",
        "np.sum(np.abs(Y_test - Y_pred))/len(Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIr3bAQgRwnp",
        "colab_type": "text"
      },
      "source": [
        "### Step 7: Refine our dataset\n",
        "* This step allows us to add or modify features of the datatset. We might do this if, for example, some combination of features better represents the problems space and so is an indicator of the target variable. \n",
        "* Here, we create one additional feature as an example, but you should reflect on our EDA earlier and see whether there are other features that can be added to our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI83luYSRwnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# here we are using pandas functionality to add a new column called LSTAT_2, which will feature values that are the square of LSTAT values\n",
        "boston_X['LSTAT_2'] = boston_X['LSTAT'].map(lambda x: x**2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md4DGMgARwnr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can run our train_test_split function and see that we have an additional features\n",
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(boston_X, boston_y, test_size = 0.2, random_state = 5)\n",
        "\n",
        "print('Number of features after Step 7: ', X_train.shape[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6hdKb4pDRwnu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can now run the same code as before on our refined dataset to see if things have improved \n",
        "lm.fit(X_train, Y_train)\n",
        "\n",
        "Y_pred = lm.predict(X_test)\n",
        "\n",
        "evaluate(Y_test, Y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXiqclcVRwnx",
        "colab_type": "text"
      },
      "source": [
        "### Step 8: Test Alternative Models\n",
        "* Once we got a nice baseline model working for this dataset, we also can try something more sophisticated and rather different, e.g. RandomForest Regressor. So, let's do so and also evaluate the result."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZ4e0MnYRwnz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# as you can see, its very similar code to instantiate the model\n",
        "# we are able to pass in additional parameters as the model is created, so optionally you can view the documentation and play with these values\n",
        "\n",
        "rfr = RandomForestRegressor()\n",
        "rfr.fit(X_train, Y_train)\n",
        "Y_pred = rfr.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TggniO1ARwn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "evaluate(Y_test, Y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3XppAfmRwn6",
        "colab_type": "text"
      },
      "source": [
        "### Step 9: Choose the best model and optimise its parameters\n",
        "* We can see that we have improved our model as we have added features and trained new models.\n",
        "* At the point that we feel comfortable with a good model, we can start to tune the parameters of the model.\n",
        "* There are a number of ways to do this, and a common way is shown below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZOHs6GtRwn7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## grid search is a 'brute force' search, one that will explore every possible combination of parameters that you provide it\n",
        "\n",
        "# we first define the parameters we want to search as a dictionary. Explore the documentation to what other options are avaiable\n",
        "params = {'n_estimators': [100, 200], 'max_depth' : [2, 10, 20]}\n",
        "\n",
        "# we then create a grid search object with our chosen model and paramters. We also use cross validation here - explored more in Day 2\n",
        "grid = model_selection.GridSearchCV(rfr, params, cv=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrtZfUzFRwn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we fit our model to the data as before\n",
        "grid.fit(X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9br0prXRwn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# one output of the grid search function is that we can get the best_estimator - the model and parameters that scored best on the training data - \n",
        "#  and save it as a new a model\n",
        "best_model = grid.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmhIn9y3RwoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# and use it to predict and evaluate as before\n",
        "Y_pred = best_model.predict(X_test)\n",
        "\n",
        "evaluate(Y_test, Y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrfGQPgLRBEd",
        "colab_type": "text"
      },
      "source": [
        "### NEXT STEPS: \n",
        "* As you can see, we have been able to try new models fairly easily and to see improvements in the evaluation metrics we have chosen. \n",
        "* If you have the opportunity, try the following:\n",
        "  * implement new models using the code structure above. Look at the `sklearn` documentation for different options\n",
        "  * explore the parameters of the different models 'by hand' (i.e. instantiate models with different parameters) and by using `GridSearchCV`\n",
        "* Develop new features for the dataset by reflecting on the EDA and determining if they might better reflect the problem we are trying to solve"
      ]
    }
  ]
}